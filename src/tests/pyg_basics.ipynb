{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch-geometric (PyG) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Basic graph data definition in PyG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data points\n",
    "Example of graph implementation in PyG as data points (`Data` class), i.e. elements of a graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# edge_index contains indexes of nodes with incident edges in the form [2, num_edges],\n",
    "# one row for the staring node index, one row for the ending node index of each edge.\n",
    "# In the following example edge_index is encoding a directed graph with three nodes\n",
    "# (0,1,2) and four edges (0->1, 1->0, 1->2, 2->1).   \n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "# Data x parameters encodes graph node features. In the following example x assign a \n",
    "# 1-dimensional feature vector to each node (x0= -1, x1= 0, x2= 1)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "# Having the edge_index and the feature matrix we can construct a PyG data point (i.e\n",
    "# a graph) with the following.\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "# NOTE: default toString shows only the size of x and edge_index  \n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Datasets\n",
    "How to import one of the graph datasets already implemented in PyG, using the `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import BAShapes\n",
    "\n",
    "# the constructor from the library return an instance of the implemented dataset chosen.\n",
    "# In this example, BAShapes() returns a Barabasi-albert (BA) graph enhanced with some motifs\n",
    "# (300 nodes and a set of 80 \"house\"-structured graphs connected to it), generated following\n",
    "# the \"GNNExplainer: Generating Explanations for Graph Neural Networks\" paper.\n",
    "dataset = BAShapes(connection_distribution=\"random\")\n",
    "print(f\"[dataset]> ...loading dataset '{dataset}' from PyG\")\n",
    "\n",
    "# a Dataset object exposes some attributes abuot the data \n",
    "print(\"\\t#entries:      \", len(dataset))\n",
    "print(\"\\t#classes:      \", dataset.num_classes)\n",
    "print(\"\\t#node_features:\", dataset.num_node_features)\n",
    "print(\"\\t#edge_features:\", dataset.num_edge_features)\n",
    "\n",
    "# a dataset entry (i.e. a graph) is retrieved as a Data object (i.e. a data point)\n",
    "graph = dataset[0]\n",
    "print(f\"\\n[dataset]> {dataset} dataset graph...\")\n",
    "print(\"\\t->\", graph)\n",
    "print(\"\\t#nodes:\", graph.num_nodes)\n",
    "print(\"\\t#edges:\", graph.num_edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparse COO Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"node_features:\", graph.x.size())\n",
    "\n",
    "edge_idx = graph.edge_index\n",
    "print(edge_idx.size)\n",
    "\n",
    "# densify a sparse COO matrix using torch\n",
    "i = edge_idx\n",
    "v = torch.ones(edge_idx.size(1))\n",
    "s = (graph.num_nodes,graph.num_nodes)\n",
    "print(v.size())\n",
    "dense = torch.sparse_coo_tensor(i, v, s).to_dense()\n",
    "print(dense.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract node neighborhood\n",
    "How to use k_hop_subgrpah()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import k_hop_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 2, 3, 4, 5], \\\n",
    "                           [2, 2, 4, 4, 6, 6]])\n",
    "\n",
    "subset, edge_index, mapping, edge_mask = k_hop_subgraph(6, 2, edge_index, relabel_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index:\n",
      " tensor([[0, 1, 2, 3],\n",
      "        [2, 2, 4, 4]])\n",
      "edge_index:\t tensor([2, 3, 4, 5, 6])\n",
      "mapping:\t tensor([4])\n"
     ]
    }
   ],
   "source": [
    "print(\"edge_index:\\n\", edge_index)\n",
    "print(\"edge_index:\\t\", subset)\n",
    "print(\"mapping:\\t\", mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss:1.9529213905334473\n",
      "epoch 1, loss:1.8417927026748657\n",
      "epoch 2, loss:1.7148265838623047\n",
      "epoch 3, loss:1.5779656171798706\n",
      "epoch 4, loss:1.439921259880066\n",
      "epoch 5, loss:1.280748963356018\n",
      "epoch 6, loss:1.1596226692199707\n",
      "epoch 7, loss:1.0392756462097168\n",
      "epoch 8, loss:0.9414781928062439\n",
      "epoch 9, loss:0.8248002529144287\n",
      "epoch 10, loss:0.7415125370025635\n",
      "epoch 11, loss:0.6432251930236816\n",
      "epoch 12, loss:0.5623167157173157\n",
      "epoch 13, loss:0.4970880150794983\n",
      "epoch 14, loss:0.4246888756752014\n",
      "epoch 15, loss:0.3953545093536377\n",
      "epoch 16, loss:0.32873061299324036\n",
      "epoch 17, loss:0.27703437209129333\n",
      "epoch 18, loss:0.2684898376464844\n",
      "epoch 19, loss:0.2486361414194107\n",
      "epoch 20, loss:0.18370674550533295\n",
      "epoch 21, loss:0.20511119067668915\n",
      "epoch 22, loss:0.16694317758083344\n",
      "epoch 23, loss:0.16266244649887085\n",
      "epoch 24, loss:0.13111881911754608\n",
      "epoch 25, loss:0.14079374074935913\n",
      "epoch 26, loss:0.11301562935113907\n",
      "epoch 27, loss:0.10305289179086685\n",
      "epoch 28, loss:0.09408984333276749\n",
      "epoch 29, loss:0.1143360510468483\n",
      "epoch 30, loss:0.10078052431344986\n",
      "epoch 31, loss:0.07009192556142807\n",
      "epoch 32, loss:0.06911405920982361\n",
      "epoch 33, loss:0.056438811123371124\n",
      "epoch 34, loss:0.06370381265878677\n",
      "epoch 35, loss:0.06787258386611938\n",
      "epoch 36, loss:0.061497293412685394\n",
      "epoch 37, loss:0.05886457860469818\n",
      "epoch 38, loss:0.06576041132211685\n",
      "epoch 39, loss:0.07080258429050446\n",
      "epoch 40, loss:0.06393811106681824\n",
      "epoch 41, loss:0.05252852663397789\n",
      "epoch 42, loss:0.06935356557369232\n",
      "epoch 43, loss:0.045095447450876236\n",
      "epoch 44, loss:0.03635799512267113\n",
      "epoch 45, loss:0.04888967052102089\n",
      "epoch 46, loss:0.04018197953701019\n",
      "epoch 47, loss:0.06246211752295494\n",
      "epoch 48, loss:0.06002379208803177\n",
      "epoch 49, loss:0.03584238886833191\n",
      "epoch 50, loss:0.03310856223106384\n",
      "epoch 51, loss:0.042185407131910324\n",
      "epoch 52, loss:0.04643095284700394\n",
      "epoch 53, loss:0.030537350103259087\n",
      "epoch 54, loss:0.027503041550517082\n",
      "epoch 55, loss:0.04222077131271362\n",
      "epoch 56, loss:0.028409605845808983\n",
      "epoch 57, loss:0.05485476925969124\n",
      "epoch 58, loss:0.03211729973554611\n",
      "epoch 59, loss:0.04691488668322563\n",
      "epoch 60, loss:0.03694150969386101\n",
      "epoch 61, loss:0.033361729234457016\n",
      "epoch 62, loss:0.0367707833647728\n",
      "epoch 63, loss:0.038843873888254166\n",
      "epoch 64, loss:0.034465473145246506\n",
      "epoch 65, loss:0.045668721199035645\n",
      "epoch 66, loss:0.03831098601222038\n",
      "epoch 67, loss:0.0353965163230896\n",
      "epoch 68, loss:0.025460021570324898\n",
      "epoch 69, loss:0.03134181722998619\n",
      "epoch 70, loss:0.034560076892375946\n",
      "epoch 71, loss:0.030941247940063477\n",
      "epoch 72, loss:0.03593090549111366\n",
      "epoch 73, loss:0.02824174240231514\n",
      "epoch 74, loss:0.037272416055202484\n",
      "epoch 75, loss:0.03802109882235527\n",
      "epoch 76, loss:0.036234453320503235\n",
      "epoch 77, loss:0.0629875510931015\n",
      "epoch 78, loss:0.0339750200510025\n",
      "epoch 79, loss:0.048181891441345215\n",
      "epoch 80, loss:0.031151607632637024\n",
      "epoch 81, loss:0.03955819830298424\n",
      "epoch 82, loss:0.03239760547876358\n",
      "epoch 83, loss:0.04650658741593361\n",
      "epoch 84, loss:0.04085464030504227\n",
      "epoch 85, loss:0.03603341057896614\n",
      "epoch 86, loss:0.039883390069007874\n",
      "epoch 87, loss:0.041750844568014145\n",
      "epoch 88, loss:0.04007997736334801\n",
      "epoch 89, loss:0.052888959646224976\n",
      "epoch 90, loss:0.04528792202472687\n",
      "epoch 91, loss:0.04874231666326523\n",
      "epoch 92, loss:0.048779428005218506\n",
      "epoch 93, loss:0.04251581057906151\n",
      "epoch 94, loss:0.03234965354204178\n",
      "epoch 95, loss:0.03893883526325226\n",
      "epoch 96, loss:0.03350801765918732\n",
      "epoch 97, loss:0.04388972744345665\n",
      "epoch 98, loss:0.025631321594119072\n",
      "epoch 99, loss:0.04339892789721489\n",
      "epoch 100, loss:0.031020021066069603\n",
      "epoch 101, loss:0.033788666129112244\n",
      "epoch 102, loss:0.0373961478471756\n",
      "epoch 103, loss:0.048258230090141296\n",
      "epoch 104, loss:0.02645835466682911\n",
      "epoch 105, loss:0.03766344115138054\n",
      "epoch 106, loss:0.047804094851017\n",
      "epoch 107, loss:0.04039952903985977\n",
      "epoch 108, loss:0.04315859079360962\n",
      "epoch 109, loss:0.032022856175899506\n",
      "epoch 110, loss:0.04810700938105583\n",
      "epoch 111, loss:0.035732872784137726\n",
      "epoch 112, loss:0.03176087886095047\n",
      "epoch 113, loss:0.023917866870760918\n",
      "epoch 114, loss:0.03622547164559364\n",
      "epoch 115, loss:0.038883026689291\n",
      "epoch 116, loss:0.03333575651049614\n",
      "epoch 117, loss:0.02269056625664234\n",
      "epoch 118, loss:0.04133845493197441\n",
      "epoch 119, loss:0.036498211324214935\n",
      "epoch 120, loss:0.0319778174161911\n",
      "epoch 121, loss:0.03630455955862999\n",
      "epoch 122, loss:0.04079024866223335\n",
      "epoch 123, loss:0.04249683767557144\n",
      "epoch 124, loss:0.03318895772099495\n",
      "epoch 125, loss:0.028525568544864655\n",
      "epoch 126, loss:0.028270797803997993\n",
      "epoch 127, loss:0.02977094054222107\n",
      "epoch 128, loss:0.02717754803597927\n",
      "epoch 129, loss:0.02723718248307705\n",
      "epoch 130, loss:0.03128591179847717\n",
      "epoch 131, loss:0.03264053538441658\n",
      "epoch 132, loss:0.045904990285634995\n",
      "epoch 133, loss:0.029086841270327568\n",
      "epoch 134, loss:0.03979193791747093\n",
      "epoch 135, loss:0.025136390700936317\n",
      "epoch 136, loss:0.034899745136499405\n",
      "epoch 137, loss:0.03208448737859726\n",
      "epoch 138, loss:0.027099547907710075\n",
      "epoch 139, loss:0.036040764302015305\n",
      "epoch 140, loss:0.031112762168049812\n",
      "epoch 141, loss:0.028658689931035042\n",
      "epoch 142, loss:0.029180176556110382\n",
      "epoch 143, loss:0.02329856902360916\n",
      "epoch 144, loss:0.04460934177041054\n",
      "epoch 145, loss:0.03613383322954178\n",
      "epoch 146, loss:0.02277401089668274\n",
      "epoch 147, loss:0.04276379942893982\n",
      "epoch 148, loss:0.03769991919398308\n",
      "epoch 149, loss:0.025950007140636444\n",
      "epoch 150, loss:0.02858581580221653\n",
      "epoch 151, loss:0.04293019697070122\n",
      "epoch 152, loss:0.018200743943452835\n",
      "epoch 153, loss:0.04135840758681297\n",
      "epoch 154, loss:0.05085664615035057\n",
      "epoch 155, loss:0.033391840755939484\n",
      "epoch 156, loss:0.03610067069530487\n",
      "epoch 157, loss:0.02426181547343731\n",
      "epoch 158, loss:0.03387684002518654\n",
      "epoch 159, loss:0.03900940343737602\n",
      "epoch 160, loss:0.03139638528227806\n",
      "epoch 161, loss:0.049875978380441666\n",
      "epoch 162, loss:0.021154232323169708\n",
      "epoch 163, loss:0.020503807812929153\n",
      "epoch 164, loss:0.036206409335136414\n",
      "epoch 165, loss:0.019026711583137512\n",
      "epoch 166, loss:0.025585880503058434\n",
      "epoch 167, loss:0.025888506323099136\n",
      "epoch 168, loss:0.034601159393787384\n",
      "epoch 169, loss:0.03968818858265877\n",
      "epoch 170, loss:0.024227214977145195\n",
      "epoch 171, loss:0.03120584227144718\n",
      "epoch 172, loss:0.022438807412981987\n",
      "epoch 173, loss:0.027297254651784897\n",
      "epoch 174, loss:0.0215163491666317\n",
      "epoch 175, loss:0.031074319034814835\n",
      "epoch 176, loss:0.028195902705192566\n",
      "epoch 177, loss:0.021550294011831284\n",
      "epoch 178, loss:0.029876142740249634\n",
      "epoch 179, loss:0.034500833600759506\n",
      "epoch 180, loss:0.052645545452833176\n",
      "epoch 181, loss:0.029351066797971725\n",
      "epoch 182, loss:0.0244885440915823\n",
      "epoch 183, loss:0.03190604969859123\n",
      "epoch 184, loss:0.024837881326675415\n",
      "epoch 185, loss:0.023066122084856033\n",
      "epoch 186, loss:0.04056161642074585\n",
      "epoch 187, loss:0.02734849415719509\n",
      "epoch 188, loss:0.022522183135151863\n",
      "epoch 189, loss:0.020858418196439743\n",
      "epoch 190, loss:0.031367186456918716\n",
      "epoch 191, loss:0.031779076904058456\n",
      "epoch 192, loss:0.020810740068554878\n",
      "epoch 193, loss:0.028206000104546547\n",
      "epoch 194, loss:0.027037663385272026\n",
      "epoch 195, loss:0.03198835626244545\n",
      "epoch 196, loss:0.02162093296647072\n",
      "epoch 197, loss:0.028928929939866066\n",
      "epoch 198, loss:0.025152388960123062\n",
      "epoch 199, loss:0.025755254551768303\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    print(f\"epoch {epoch}, loss:{loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('masterXAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4559d4a6aa1c4a5efdbe8a44294c257da884b23ec5d8d2eeed237201549e4e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
